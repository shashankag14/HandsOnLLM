{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction-Based Prompting\n",
    "This is the most straightforward form of prompting, where you give the model clear, direct instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT\n",
    "query = \"List three benfits of yoga\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(messages)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Context Learning\n",
    "In-context learning is where you provide examples in the prompt to guide the model on the desired format or tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT\n",
    "few_shot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Classify the price of the item\"},\n",
    "    {\"role\": \"user\", \"content\": \"Yacht\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Expensive\"},\n",
    "    {\"role\": \"user\", \"content\": \"Pencil\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Cheap\"},\n",
    "    {\"role\": \"user\", \"content\": \"Diamond\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Expensive\"},\n",
    "    {\"role\": \"user\", \"content\": \"Burger\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(few_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Prompting\n",
    "Chain prompting is about breaking down a task into a sequence of smaller tasks, where each answer informs the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a startup idea\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Suggest one real world problem to focus to build a startup\"}\n",
    "]\n",
    "startup_idea = pipe(messages)[0][\"generated_text\"]\n",
    "startup_idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get startup's name and slogan \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Suggest a catchy name and slogan for the following startup idea: {startup_idea}\"}\n",
    "]\n",
    "startup_description = pipe(messages)[0][\"generated_text\"]\n",
    "startup_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create funding pitch for startup\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Create a small pitch to get funding for {startup_description}\"}\n",
    "]\n",
    "startup_pitch = pipe(messages)[0][\"generated_text\"]\n",
    "startup_pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Reasoning\n",
    "Chain of thought (CoT) prompting encourages the model to \"think aloud,\" or provide a step-by-step reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answering with chain-of-thought\n",
    "cot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\"},\n",
    "    {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(cot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree of Thought Reasoning\n",
    "Tree of thought (ToT) involves exploring multiple reasoning paths to reach the best outcome. Think of it like a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree-of-Thought Prompt\n",
    "zeroshot_tot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Imagine two different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. There shall be a maximum of 4 steps. If any expert realises they're wrong at any point then they leave. The question is 'Which company is better for investing 1000 dollars? Nvidia or AMD.' Make sure to discuss the results.\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "outputs = pipe(zeroshot_tot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Consistency\n",
    "Self-consistency involves generating multiple responses and choosing the most common answer to reduce variability and improve reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self consistency prompt\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"If 1+5 = -6 and 2+3 = -5, how much is 4+2? Do not explain, just give the answer\"}\n",
    "]\n",
    "\n",
    "# Store all the responses after multiple runs\n",
    "responses = [pipe(query)[0]['generated_text'] for _ in range(3)]\n",
    "print(\"Answers:\", responses)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c095b99d13058df104963e08c2b9e0380c987cd328ed3d2597f60f6d2207d521"
  },
  "kernelspec": {
   "display_name": "Python 3.10.15 ('thellmbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
