{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate the output\n",
    "output = pipe(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Sampling in Action\n",
    "To see temperature sampling in practice, let‚Äôs try generating a joke about chickens with different temperature settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Temperature = 0 (Conservative Output)\n",
    "Using a low temperature keeps the model focused on the most likely response, giving a more traditional, straightforward joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a high temperature\n",
    "output = pipe(messages, do_sample=False, temperature=0.0)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Temperature = 1 (Creative Output)\n",
    "Raising the temperature makes the model explore a wider range of responses, resulting in a more creative joke!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a high temperature\n",
    "output = pipe(messages, do_sample=True, temperature=1.0)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P (Nucleus) Sampling in Action\n",
    "With Top-P sampling, we let the model choose words until it reaches a cumulative probability threshold, ùëù. This gives a dynamic balance between predictability and creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Low Top-P (P=0.5)\n",
    "With a low P, the model only picks from highly probable words, resulting in a focused, straightforward response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a low top-p value (focused response)\n",
    "output = pipe(messages, do_sample=True, top_p=0.5)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Higher Top-P (P=2.0)\n",
    "A higher P allows the model to consider a broader range of options, making the response more expressive and detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a high top-p value (more varied response)\n",
    "output = pipe(messages, do_sample=True, top_p=2.0)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling in Action\n",
    "In top-k sampling, we limit the model‚Äôs options to the top K most likely words. This keeps responses concise or, if K is set higher, more varied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Low Top-K (K=5)\n",
    "With a small K, the model chooses only from the five most probable next words, making the response more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a low top-k value (concise response)\n",
    "output = pipe(messages, do_sample=True, top_k=5)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Higher Top-K (K=40)\n",
    "A larger K value allows the model to consider more options, leading to a more varied and creative response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a high top-k value (more creative response)\n",
    "output = pipe(messages, do_sample=True, top_k=40)\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c095b99d13058df104963e08c2b9e0380c987cd328ed3d2597f60f6d2207d521"
  },
  "kernelspec": {
   "display_name": "Python 3.10.15 ('thellmbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
