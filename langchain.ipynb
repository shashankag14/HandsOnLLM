{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/bin/pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
    "!/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/bin/pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Phi-3 Mini\n",
    "Fetch the compressed Phi-3 model with half precision of FP16 compared to the original model with full precision of FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=0,  # All the layers of the model run on the GPU\n",
    "    max_tokens=500,   # limits the output length (how much text the model will generate in response).\n",
    "    n_ctx=2048,       # determines the input capacity (how much context the model can handle).\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chain\n",
    "Combines Prompt with LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  self._input_ids.tolist(), prompt_tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' As of my knowledge cutoff date in 2023, Germany is comprised of 16 federal states, known as Bundesländer. These states each have their own government and certain powers delegated to them by the German federal system. The states play a significant role in various areas such as education, law enforcement, and cultural affairs within their territories.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"<s><|user|>{input_prompt}<|end|><|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")\n",
    "\n",
    "# Connect the prompt module with the model\n",
    "basic_chain = prompt | llm\n",
    "\n",
    "# Input the prompt\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "    \"input_prompt\": \"How many states are there in Germany?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Chains\n",
    "Combines multiple modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  self._input_ids.tolist(), prompt_tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idea': 'An old lady is knitting a pullover while humming a tune',\n",
       " 'product_description': ' Yarn for cozy sweaters.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "product_template = \"\"\"<s><|user|>Suggest one product in 2-5 words which could be advertised based on a short description: {idea}. Only return the product which could be advertised.<|end|><|assistant|>\"\"\"\n",
    "product_prompt = PromptTemplate(\n",
    "    template=product_template,\n",
    "    input_variables=[\"idea\"]\n",
    ")\n",
    "product_description = LLMChain(llm=llm, prompt=product_prompt, output_key=\"product_description\")\n",
    "product_description.invoke(\n",
    "    {\"idea\": \"An old lady is knitting a pullover while humming a tune\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_slogan_template = \"\"\"<s><|user|>Suggest one name and one slogan for the product based on the idea: {idea} and product: {product_description}<|end|><|assistant|>\"\"\"\n",
    "product_name_slogan_prompt= PromptTemplate(\n",
    "    template=product_name_slogan_template,\n",
    "    input_variables=[\"idea\", \"product_description\"]\n",
    ")\n",
    "product_name_slogan = LLMChain(llm=llm, prompt=product_name_slogan_prompt, output_key=\"product_name_slogan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ad_template = \"\"\"<s><|user|>Give a really short advertisement idea to pitch to the team in 4-5 sentences based on the idea: {idea}, product description: {product_description}. Also, consider the name and slogan of the product, which is: {product_name_slogan}<|end|><|assistant|>\"\"\"\n",
    "product_ad_prompt = PromptTemplate(\n",
    "    template=product_ad_template,\n",
    "    input_variables=[\"idea\", \"product_name_slogan\", \"product_description\"]\n",
    ")\n",
    "\n",
    "product_ad = LLMChain(llm=llm, prompt=product_ad_prompt, output_key=\"product_ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  self._input_ids.tolist(), prompt_tokens\n",
      "/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  self._input_ids.tolist(), prompt_tokens\n",
      "/Users/shashankagarwal/opt/anaconda3/envs/thellmbook/lib/python3.10/site-packages/llama_cpp/llama.py:1031: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  self._input_ids.tolist(), prompt_tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idea': 'An old lady is knitting a pullover while humming a tune',\n",
       " 'product_description': ' Yarn or Knitting Needles Set!',\n",
       " 'product_name_slogan': ' Name: \"Knit-Nurture Essentials\"\\n\\nSlogan: \"Stitch by Stitch, Crafting Comfort for Every Season!\"',\n",
       " 'product_ad': ' Introducing \"Knit-Nurture Essentials,\" a Yarn or Knitting Needles Set designed with the crafter in mind. Picture our beloved grandmother, knitting cozily while humming along to her favorite tunes—now imagine bringing that comfort and warmth into every room of your home. With \"Stitch by Stitch, Crafting Comfort for Every Season!\" at hand, create a personalized pullover just like our inspiration. Join the Knit-Nurture family today and knit your way to a world full of homemade charm!'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_chain = product_description | product_name_slogan | product_ad\n",
    "product_chain.invoke(\"An old lady is knitting a pullover while humming a tune\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c095b99d13058df104963e08c2b9e0380c987cd328ed3d2597f60f6d2207d521"
  },
  "kernelspec": {
   "display_name": "Python 3.10.15 ('thellmbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
