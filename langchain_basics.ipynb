{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
    "# !pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Phi-3 Mini\n",
    "Fetch the compressed Phi-3 model with half precision of FP16 compared to the original model with full precision of FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"wget\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "# !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"models/Phi-3-mini-4k-instruct-fp16.gguf\", # Path to model\n",
    "    n_gpu_layers=-1,  # All the layers of the model run on the GPU\n",
    "    max_tokens=500,   # limits the output length (how much text the model will generate in response).\n",
    "    n_ctx=2048,       # determines the input capacity (how much context the model can handle).\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chain\n",
    "Combines Prompt with LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Germany is a federal republic comprising 16 states, known as \"Bundesländer\" in German. These states are Baden-Württemberg, Bavaria (Bayern), Berlin, Brandenburg, Bremen, Hamburg, Hesse (Hessen), Mecklenburg-Vorpommern, North Rhine-Westphalia (Nordrhein-Westfalen), Saarland, Saxony (Sachsen), Saxony-Anhalt (Sachsen-Anhalt), Schleswig-Holstein, Thuringia (Thüringen), Lower Saxony (Niedersachsen), and Upper Saxony (Oberlausitz).'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"<s><|user|>{input_prompt}<|end|><|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")\n",
    "\n",
    "# Connect the prompt module with the model\n",
    "basic_chain = prompt | llm\n",
    "\n",
    "# Input the prompt\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "    \"input_prompt\": \"How many states are there in Germany?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Chains\n",
    "Combines multiple modules together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagarwa2\\AppData\\Local\\Temp\\ipykernel_12392\\1635870151.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  product_description = LLMChain(llm=llm, prompt=product_prompt, output_key=\"product_description\")\n",
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idea': 'An old lady is knitting a pullover while humming a tune',\n",
       " 'product_description': ' Cozy knitted pullover yarn.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "product_template = \"\"\"<s><|user|>Suggest one product in 2-5 words which could be advertised based on a short description: {idea}. Only return the product which could be advertised.<|end|><|assistant|>\"\"\"\n",
    "product_prompt = PromptTemplate(\n",
    "    template=product_template,\n",
    "    input_variables=[\"idea\"]\n",
    ")\n",
    "product_description = LLMChain(llm=llm, prompt=product_prompt, output_key=\"product_description\")\n",
    "product_description.invoke(\n",
    "    {\"idea\": \"An old lady is knitting a pullover while humming a tune\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_slogan_template = \"\"\"<s><|user|>Suggest one name and one slogan for the product based on the idea: {idea} and product: {product_description}<|end|><|assistant|>\"\"\"\n",
    "product_name_slogan_prompt= PromptTemplate(\n",
    "    template=product_name_slogan_template,\n",
    "    input_variables=[\"idea\", \"product_description\"]\n",
    ")\n",
    "product_name_slogan = LLMChain(llm=llm, prompt=product_name_slogan_prompt, output_key=\"product_name_slogan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ad_template = \"\"\"<s><|user|>Give a really short advertisement idea to pitch to the team in 4-5 sentences based on the idea: {idea}, product description: {product_description}. Also, consider the name and slogan of the product, which is: {product_name_slogan}<|end|><|assistant|>\"\"\"\n",
    "product_ad_prompt = PromptTemplate(\n",
    "    template=product_ad_template,\n",
    "    input_variables=[\"idea\", \"product_name_slogan\", \"product_description\"]\n",
    ")\n",
    "\n",
    "product_ad = LLMChain(llm=llm, prompt=product_ad_prompt, output_key=\"product_ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'idea': 'An old lady is knitting a pullover while humming a tune',\n",
       " 'product_description': ' Cozy knitted pullover!',\n",
       " 'product_name_slogan': ' Name: \"Elderly Elegance Wrap\"\\n\\nSlogan: \"Warmth in Every Stitch - Embracing Comfort and Style, One Thread at a Time.\"',\n",
       " 'product_ad': ' Introducing \"Elderly Elegance Wrap\" - a cozy knitted pullover that brings comfort and style to your loved ones. Picture an elderly lady, serenely knitting her signature piece with a smile on her face as she hums a familiar tune. Our unique product is not just another pullover; it\\'s warmth in every stitch - embracing comfort and style one thread at a time! Experience the essence of Elderly Elegance Wrap, where timeless craftsmanship meets modern elegance. Don\\'t miss out on this must-have for your loved ones\\' cozy winter collection!'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_chain = product_description | product_name_slogan | product_ad\n",
    "product_chain.invoke(\"An old lady is knitting a pullover while humming a tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_template=\"\"\"<s><|user|>Summarize the conversation and update with new lines.\n",
    "Current summary: {summary}\n",
    "New lines: {new_lines}\n",
    "Updated summary: <|end|><|assistant|>\n",
    "\"\"\"\n",
    "summary_prompt=PromptTemplate(\n",
    "    template=summary_template,\n",
    "    input_variables=[\"summary\", \"new_lines\"]\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, prompt=summary_prompt, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hey, I am visiting Germany in September. Do you know any breweries to try some nice beer?',\n",
       " 'chat_history': '',\n",
       " 'text': ' Absolutely! Germany is known as the beer paradise, and September offers fantastic opportunities to explore various breweries. Here are a few must-visit ones:\\n\\n1. Augustiner Brewery in Munich - Located near Marienplatz, this traditional Bavarian brewery has been producing beer since 1328. Try their renowned Augustiner Helles lager and enjoy the festive atmosphere at Oktoberfest (although it takes place earlier in September).\\n\\n2. Hofbrauhaus Munich - Another iconic Munich-based brewery with a rich history dating back to 1589. They offer tours, traditional Bavarian food, and have several beer gardens where you can taste their famous Märzen lager.\\n\\n3. Weihenstephan Brewery in Freising - As one of the oldest breweries in the world (founded in 768), they\\'re also located within walking distance from Munich, making it convenient to visit both places on a day trip. They offer various tours and tastings of their popular beers, such as Märzen.\\n\\n4. Bräustüberl Weinstube Göttersburg in Bamberg - In the beautiful town of Bamberg, this family-owned brewery serves an array of local specialty beer styles like Franconian Altbier and Heller Bock. Try their regional sausages while enjoying a drink!\\n\\n5. Paulaner Brauerei in Munich - This historic monastery-brewery offers tours where you can learn about the brewing process, try different types of beers, and enjoy views over the Isar River. They have several bars within their complex to taste their diverse selection.\\n\\n6. Spaten Brewery Am Glockenspiel in Munich - With a rich history dating back to 1397, this brewery is home to an iconic beer garden and offers various tours where you can sample their unique beers like Spaten Optimator lager or Doppelbock.\\n\\n7. Braugasthaus \"Katz\" in Nuremberg - Located in the charming old town, this brewery has been around since 1529 and serves an ass'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple prompt\n",
    "template = \"\"\"<s><|user|>Current conversation: {chat_history}\n",
    "{input_prompt}<|end|><|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")\n",
    "\n",
    "# Link LLM chain to conversation memory\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "llm_chain.invoke({\"input_prompt\": \"Hey, I am visiting Germany in September. Do you know any breweries to try some nice beer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "c:\\Users\\sagarwa2\\AppData\\Local\\miniconda3\\envs\\thellmbook\\lib\\site-packages\\llama_cpp\\llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Which month am I traveling to Germany?',\n",
       " 'chat_history': ' While visiting Germany in September, you\\'re interested in exploring various breweries to enjoy some nice beer. Here are seven must-visit brewery destinations:\\n\\n1. Augustiner Brewery in Munich offers traditional Bavarian experiences with its Augustiner Helles lager and Oktoberfest tours.\\n2. Hofbrauhaus Munich, dating back to 1589, provides tours, food, and beer gardens for their Märzen lager.\\n3. Weihenstephan Brewery in Freising is one of the world\\'s oldest breweries (founded in 768), offering diverse tastings near Munich.\\n4. Bräustüberl Weinstube Göttersburg in Bamberg serves local specialties like Franconian Altbier and Heller Bock, with regional sausages on the menu.\\n5. Paulaner Brauerei in Munich has an engaging tour where visitors can learn about brewing and taste a wide variety of beers.\\n6. Spaten Brewery Am Glockenspiel showcases its long history by providing tours, beer garden experiences, and unique beers like Spaten Optimator lager or Doppelbock.\\n7. Braugasthaus \"Katz\" in Nuremberg dates back to 1529, offering an authentic experience within the city\\'s charming old town.',\n",
       " 'text': ' You are traveling to Germany in September.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\n",
    "    {\"input_prompt\": \"Which month am I traveling to Germany?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
